{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Job Vacancies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this project, we'll build a web scraper to extract job listings from a popular job search platform. We'll extract job titles, companies, locations, job descriptions, and other relevant information.\n",
    "\n",
    "Here are the main steps we'll follow in this project:\n",
    "\n",
    "1. Setup our development environment\n",
    "2. Understand the basics of web scraping\n",
    "3. Analyze the website structure of our job search platform\n",
    "4. Write the Python code to extract job data from our job search platform\n",
    "5. Save the data to a CSV file\n",
    "6. Test our web scraper and refine our code as needed\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this project, you should have some basic knowledge of Python programming and HTML structure. In addition, you may want to use the following packages in your Python environment:\n",
    "\n",
    "- requests\n",
    "- BeautifulSoup\n",
    "- csv\n",
    "- datetime\n",
    "\n",
    "These packages should already be installed in Coursera's Jupyter Notebook environment, however if you'd like to install additional packages that are not included in this environment or are working off platform you can install additional packages using `!pip install packagename` within a notebook cell such as:\n",
    "\n",
    "- `!pip install requests`\n",
    "- `!pip install BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.25.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests) (2024.7.4)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.26.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (24.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Collecting webdriver_manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\anaconda3\\lib\\site-packages (from webdriver_manager) (2.32.2)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\hp\\anaconda3\\lib\\site-packages (from webdriver_manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from webdriver_manager) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests->webdriver_manager) (2024.7.4)\n",
      "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Installing collected packages: webdriver_manager\n",
      "Successfully installed webdriver_manager-4.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas matplotlib seaborn selenium\n",
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Realizar la solicitud HTTP al sitio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def enter_job_url():\n",
    "    while  True:\n",
    "        try:\n",
    "            job = input('Ingresa el trabajo que quieres buscar: ').strip()\n",
    "            listjob = job.split()\n",
    "\n",
    "# Podria mejorarse para filtrar por ubicación\n",
    "# url = 'https://co.computrabajo.com/' + 'trabajo-de-' + \"-\".join(listjob) + 'de-' + location\n",
    "# Tambien se prodria verificar en una lista o base de datos solo por nombre del trabajo y location para filtrar y no buscar ofertas que no corresponden        \n",
    "            url = 'https://co.computrabajo.com/' + 'trabajo-de-' + \"-\".join(listjob)\n",
    "\n",
    "        \n",
    "            print(f\"URL a buscar: {url}\")\n",
    "            return url\n",
    "        except ValueError as ve:\n",
    "            print(ve)\n",
    "        except Exception as e:\n",
    "            print(f\"Error Inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analizar el HTML y guardar los datos en un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Ingresa el trabajo que quieres buscar:  profesor de fisica\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL a buscar: https://co.computrabajo.com/trabajo-de-profesor-de-fisica\n",
      "Total de ofertas: 175\n",
      "Datos guardados en Job_Offers.csv\n"
     ]
    }
   ],
   "source": [
    "def analysis_HTML():\n",
    "    # Obtén la URL para buscar el trabajo\n",
    "    url = enter_job_url()\n",
    "\n",
    "    # Configurar Selenium y abrir la página web\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Esperar explícitamente hasta que los elementos carguen (máximo 20 segundos)\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'box_offer'))\n",
    "        )\n",
    "    except:\n",
    "        print(\"No se pudo cargar la página a tiempo.\")\n",
    "        driver.quit()\n",
    "        return\n",
    "\n",
    "    # Extraer ofertas\n",
    "    def extract_offers():\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Usar una lista para almacenar la información de las ofertas\n",
    "        job_list = []\n",
    "        \n",
    "         # Extraer el número total de ofertas desde el h1 dentro del div box_title\n",
    "        num_offers = soup.select_one('div.box_title h1.title_page span.fwB')\n",
    "\n",
    "        if num_offers:\n",
    "            # Obtener el texto dentro del span y procesarlo\n",
    "            num_offers_text = num_offers.text.strip()\n",
    "\n",
    "            try:\n",
    "                # Intentar convertir a número entero\n",
    "                total_offers = int(num_offers_text.replace('.', ''))\n",
    "                print(f\"Total de ofertas: {total_offers}\")\n",
    "            except ValueError:\n",
    "                print(f\"Error al convertir a entero: '{num_offers_text}' no es un número válido.\")\n",
    "        else:\n",
    "            print(\"No se encontró el número de ofertas.\")\n",
    "\n",
    "# Iterar sobre los primeros 20 artículos, porque no hace el cambio de pagina-- a mejorar\n",
    "        for i in range(total_offers):\n",
    "            offer_tag = soup.find('article', {'data-lc': f'ListOffers-Score4-{i}'})\n",
    "            \n",
    "            if not offer_tag:\n",
    "                continue  # Si no encuentra la oferta, pasa a la siguiente\n",
    "\n",
    "            # Extraer título, enlace, empresa, ubicación, salario y tiempo de publicación\n",
    "            title_tag = offer_tag.find('h2', class_='fs18 fwB')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else 'No disponible'\n",
    "\n",
    "            link_tag = offer_tag.find('a', class_='js-o-link fc_base')\n",
    "            link = link_tag['href'] if link_tag else 'No disponible'\n",
    "\n",
    "            company_tag = offer_tag.find('a', class_='fc_base t_ellipsis')\n",
    "            company = company_tag.get_text(strip=True) if company_tag else 'No disponible'\n",
    "\n",
    "            location_tag = offer_tag.find('p', class_='fs16 fc_base mt5')\n",
    "            location = location_tag.get_text(strip=True) if location_tag else 'No disponible'\n",
    "\n",
    "            salary_tag = offer_tag.find('div', class_='fs13 mt15')\n",
    "            salary = salary_tag.get_text(strip=True) if salary_tag else 'No especificado'\n",
    "\n",
    "            time_posted_tag = offer_tag.find('p', class_='fs13 fc_aux mt15')\n",
    "            time_posted = time_posted_tag.get_text(strip=True) if time_posted_tag else 'No disponible'\n",
    "\n",
    "            # Agregar los datos a la lista de ofertas\n",
    "            job_list.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'salary': salary,\n",
    "                'time_posted': time_posted\n",
    "            })\n",
    "\n",
    "        return job_list\n",
    "\n",
    "    # Guardar datos en un archivo CSV\n",
    "    def save_csv():\n",
    "        offers = extract_offers()\n",
    "        if offers:\n",
    "            df = pd.DataFrame(offers)\n",
    "            df.to_csv('Job_Offers.csv', index=False)\n",
    "            print(\"Datos guardados en Job_Offers.csv\")\n",
    "        else:\n",
    "            print(\"No se encontraron ofertas para guardar.\")\n",
    "\n",
    "    save_csv()\n",
    "    driver.quit()\n",
    "\n",
    "analysis_HTML()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Visualizar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"PASOS\n",
    "leer el csv\n",
    "extraer los datos a imprimir:\n",
    "    1. titulo de la oferta\n",
    "    2. Company\n",
    "    3. ubicacion  ---- la misma si se aplica el filtro\n",
    "    4. salario\n",
    "    5. el timepo de publicacion\n",
    "\n",
    "cual es el grafico mas optimo para mostrar los datos?\n",
    "que quiero mostrar?\n",
    "el salario de cada oferta?\n",
    "que pasa con la experiencia? ------ necesito saber cuanta experiencia solicitan en cada oferta para eso hay que aplicar el filtro \n",
    "\n",
    "reconocer las limitaciones de la pagina de empleos por que algunos filtros no aplican correctamente, por que no aplican sobre las\n",
    "descripciones de cada oferta, sino por indicación del coampañio u ofertante. \n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
